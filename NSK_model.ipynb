{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuoseklių sakinių klasifikavimas atsitiktinių imčių medicininių tyrimų santraukose (galutinis modelis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Dokumente \"[**Neural Networks for Joint Sentence Classification in Medical Paper Abstracts**](https://arxiv.org/pdf/1612.05251)\" sudarytas ANN modelis pasiekė 89.9% tikslumą po mokymo su visais **PubMed 20k RCT** rinkinio pavyzdžiais. Mūsų geriausias Modelis 5, mokytas su 10% **PubMed 20k RCT** pavyzdžių, santraukų sakinių klasifikaciją atlieka ~85.6% tikslumu.\n",
    "\n",
    "Šiame etape sudarysim NSK Modelį, kuris bus eksperimentų metu sudarytas Modelis 5 apmokytas su visais **PubMed 20k RCT** rinkinio pavyzdžiais, mokymą atliekant daugiau nei 5 epochas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotekų importavimas\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# išjungiam TensorFlow įspėjimų (warnings) vaizdavimą rezultatuose\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# glabalios TensorFlow random seed reikšmės nustatymas\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagalbinių funkcijų importavimas\n",
    "from helper_functions.dataset_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrai gauti eksperimentavimo metu\n",
    "\n",
    "# santraukos sakinio išskirstyto žodžiais vektoriaus dydis\n",
    "output_sentence_len = 55\n",
    "\n",
    "# santraukos sakinio išskirstyto simboliais vektoriaus dydis\n",
    "output_sentence_chars_len = 290\n",
    "\n",
    "# simbolių embedding'o vetoriaus dydis\n",
    "max_char_tokens = 28\n",
    "\n",
    "# dažniausiai psikartojančių sakinių numerių dydis\n",
    "line_numbers_depth = 17\n",
    "\n",
    "# didžiausia dalį santraukų sudarančių sakinių sakičius\n",
    "sentences_total_depth = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Duomenų paruošimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame sudarymas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duomenų rinkinio tekstinių failų nuskaitymas\n",
    "DATASET_PATH = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "\n",
    "_, train_dicts = convert_file_lines_to_dic(DATASET_PATH + 'train.txt')\n",
    "_, test_dicts = convert_file_lines_to_dic(DATASET_PATH + 'test.txt')\n",
    "_, val_dicts = convert_file_lines_to_dic(DATASET_PATH + 'dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas DataFrame sudarymas\n",
    "train_df = pd.DataFrame(train_dicts)\n",
    "test_df = pd.DataFrame(test_dicts)\n",
    "val_df = pd.DataFrame(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sakinių sąrašų sudarymas\n",
    "X_train = train_df[\"text\"].to_list()\n",
    "X_test = test_df[\"text\"].to_list()\n",
    "X_val = val_df[\"text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sakinių išskaidytų simboliais sąrašas\n",
    "X_train_chars = [split_sentence_chars(sentence) for sentence in X_train]\n",
    "X_test_chars = [split_sentence_chars(sentence) for sentence in X_test]\n",
    "X_val_chars = [split_sentence_chars(sentence) for sentence in X_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duomenų formato pakeitimas (encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sakinių etikėčių kodavimas su LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_label = label_encoder.fit_transform(train_df[\"target\"])\n",
    "y_test_label = label_encoder.transform(test_df[\"target\"])   \n",
    "y_val_label = label_encoder.transform(val_df[\"target\"])\n",
    "\n",
    "etiketes = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sakinių etikėčių kodavimas TensorFlow modeliui su OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "y_train_onehot = onehot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "y_test_onehot = onehot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))   \n",
    "y_val_onehot = onehot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sakinių numerių kodavimas su TensorFlow OneHot nurodant gylį\n",
    "train_line_numbers_onehot = tf.one_hot(train_df[\"number\"].to_numpy(), depth=line_numbers_depth)\n",
    "test_line_numbers_onehot = tf.one_hot(test_df[\"number\"].to_numpy(), depth=line_numbers_depth)\n",
    "val_line_numbers_onehot = tf.one_hot(val_df[\"number\"].to_numpy(), depth=line_numbers_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# santraukų sakinių skaičiaus kodavimas su TensorFlow OneHot nurodant gylį\n",
    "train_sentences_total_onehot = tf.one_hot(train_df[\"total\"].to_numpy(), depth=sentences_total_depth)\n",
    "test_sentences_total_onehot = tf.one_hot(test_df[\"total\"].to_numpy(), depth=sentences_total_depth)\n",
    "val_sentences_total_onehot = tf.one_hot(val_df[\"total\"].to_numpy(), depth=sentences_total_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Dataset sudarymas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Dataset sudarymas\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_onehot,\n",
    "                                                 train_sentences_total_onehot,\n",
    "                                                 X_train,\n",
    "                                                 X_train_chars))\n",
    "\n",
    "train_labels = tf.data.Dataset.from_tensor_slices(y_train_onehot)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((train_data, train_labels)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Dataset sudarymas\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_onehot,\n",
    "                                                test_sentences_total_onehot,\n",
    "                                                X_test,\n",
    "                                                X_test_chars))\n",
    "\n",
    "test_labels = tf.data.Dataset.from_tensor_slices(y_test_onehot)\n",
    "\n",
    "test_dataset = tf.data.Dataset.zip((test_data, test_labels)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation TensorFlow Dataset sudarymas\n",
    "val_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_onehot,\n",
    "                                               val_sentences_total_onehot,\n",
    "                                               X_val,\n",
    "                                               X_val_chars))\n",
    "\n",
    "val_labels = tf.data.Dataset.from_tensor_slices(y_val_onehot)\n",
    "\n",
    "val_dataset = tf.data.Dataset.zip((val_data, val_labels)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelio sudarymas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Žodžių vektorizacijos sluoksnio paruošimas (Universal Sentence Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 11:09:28.763763: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 11:09:29.276377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4653 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# importuojam TensorFlow Hub modulį\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# atsisiunčiam if TF Hub Universal Sentence Encoder\n",
    "word_tokens_embedder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", # nuoroda į iš anksto apmokyta modelį\n",
    "                                            trainable=False, # nurodome nekeisti modelio išmoktų savybių\n",
    "                                            name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Simbolių vektorizacijos sluoksnio paruošimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sukuriam vektorizavimo sluoksnį\n",
    "char_vectorizer = layers.TextVectorization(max_tokens=max_char_tokens,\n",
    "                                           output_sequence_length=output_sentence_chars_len,\n",
    "                                           name=\"char_vectorizer\")\n",
    "\n",
    "# sukuriam tokenizacijos žodyną naudojant į simbolius išskaidytus X_train_chars sakinius\n",
    "char_vectorizer.adapt(X_train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizacijos metu sudaryto žodyno apžvalga\n",
    "char_tokens_vocabulary = char_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sukuriam simbolių embedding'o sluoksnį\n",
    "char_tokens_embedder_layer = layers.Embedding(input_dim=len(char_tokens_vocabulary),\n",
    "                                              output_dim=24, # rezultato vektoriaus dydis\n",
    "                                              mask_zero=True, # paslepiam vektorizuoto sakinio tuščias reikšmes\n",
    "                                              name=\"char_tokens_embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelio mokymas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard callback funkcijos sudarymas\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"tensorboard\",  # aplankas kur saugoti log failus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint callback funkcijos sudarymas\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"checkpoints/m6.ckpt\", # failo, kuriame saugosime atskaitos tašką pavadinimas\n",
    "    monitor=\"val_accuracy\",         # stebimas parametras, kurio atžvilgiu kursime taškus (įvertinimo tikslumas)\n",
    "    verbose=0,                      # išjungiam funkcijos pranešimus\n",
    "    save_best_only=True,            # nurodom saugoti tik modelį esant geriausiam stebimam parametrui\n",
    "    save_weights_only=True,         # saugojam tik įverčius, bet ne visą modelį\n",
    "    save_freq='epoch',              # kaip dažnai vertinti stebimą parametrą\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping callback fnkcijos sudarymas\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",         # parametras, kurio pokyčiai bus stebimi\n",
    "    min_delta=0.001,            # dydis nurodantis koks mažiausias stebimo parametro teigiamas pokytis yra priimtinas\n",
    "    patience=5,                 # kiek epochų laukti stebimo parametro pokyčio, prieš sustabdant mokymą\n",
    "    verbose=0,                  # išjungiam funkcijos pranešimus\n",
    "    restore_best_weights=True,  # atstatyti modelio stadiją su geriausiasiais mokymosi rezultatais\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelio 5 klonavimas\n",
    "model_6 = tf.keras.models.clone_model(model_5)\n",
    "model_6.set_weights(model_5.get_weights())\n",
    "model_6._name = \"model_6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# išvedam sudaryto modelio struktūrą\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sukūriam (kompiliuojam) aprašytą modelį\n",
    "model_6.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudarom modelio \"Modelis 6\" vertinimo diagramas\n",
    "plot_accuracy_loss_curves(model_6_history, \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# įvertinam modelį su visais testavimo rinkinio pavyzdžiais\n",
    "# su visais training 20k pavyzdziu apmokytas modelis evaluatina 85.15%\n",
    "loss, accuracy = model_6.evaluate(test_tokens_chars_pos_dataset)\n",
    "print(f\"Modelio 6 tikslumas su testavimo duomenimis: {(accuracy*100):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
